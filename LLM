from langchain.chat_models import AzureChatOpenAI
from langchain.openai import AzureOpenAIEmbeddings
import os
from dotenv import load_dotenv
import os
# Load environment variables
load_dotenv()

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")

# Set up the environment variables as shown in the original code
os.environ["AZURE_OPENAI_API_KEY"] = AZURE_OPENAI_API_KEY
os.environ["AZURE_OPENAI_ENDPOINT"] = AZURE_OPENAI_ENDPOINT

# Initialize Azure OpenAI model
llm = AzureChatOpenAI(
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_key=AZURE_OPENAI_API_KEY,
    openai_api_version="2025-01-01-preview",
    azure_deployment="gpt-4o",
    openai_api_type="azure",
)

# Initialize Azure OpenAI embeddings
embedding_model = AzureOpenAIEmbeddings(
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_key=AZURE_OPENAI_API_KEY,
    openai_api_version="2023-05-15",
    azure_deployment="text-embedding-ada-002",
)

# Import necessary libraries for document handling
from langchain.document_loaders import WebBaseLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class DocumentHandler:
    """Class to handle document loading and processing"""
    
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    
    def load_document(self, url):
        """Load document from URL based on file type"""
        if url.endswith('.pdf'):
            loader = PyPDFLoader(url)
        else:
            loader = WebBaseLoader(url)
        
        return loader.load()
    
    def chunk_documents(self, documents):
        """Split documents into chunks"""
        return self.text_splitter.split_documents(documents)


class VectorStoreManager:
    """Class to manage vector store operations"""
    
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        self.vectorstore = None
    
    def create_vectorstore(self, documents):
        """Create vector store from documents"""
        self.vectorstore = FAISS.from_documents(documents, self.embedding_model)
        return self.vectorstore
    
    def get_retriever(self, k=5):
        """Get retriever from vector store"""
        if self.vectorstore is None:
            raise ValueError("Vector store not initialized")
        
        return self.vectorstore.as_retriever(search_kwargs={"k": k})


class DocumentScorer:
    """Class to score documents based on relevance"""
    
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def calculate_similarity_score(self, query, document):
        """Calculate similarity score between query and document"""
        import numpy as np
        
        # Get embeddings
        query_embedding = self.embedding_model.embed_query(query)
        doc_embedding = self.embedding_model.embed_documents([document.page_content])[0]
        
        # Calculate cosine similarity
        similarity = np.dot(query_embedding, doc_embedding) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
        )
        
        return float(similarity)
    
    def score_documents(self, query, documents):
        """Score a list of documents based on query relevance"""
        scored_docs = []
        
        for doc in documents:
            score = self.calculate_similarity_score(query, doc)
            
            scored_docs.append({
                "document": doc,
                "score": score,
                "source": doc.metadata.get("source", "Unknown")
            })
        
        # Sort by score (descending)
        return sorted(scored_docs, key=lambda x: x["score"], reverse=True)


class RAGPipeline:
    """Main RAG Pipeline class integrating all components"""
    
    def __init__(self, llm, embedding_model):
        self.llm = llm
        self.embedding_model = embedding_model
        self.doc_handler = DocumentHandler()
        self.vector_manager = VectorStoreManager(embedding_model)
        self.doc_scorer = DocumentScorer(embedding_model)
        
    def process_documents(self, document_urls):
        """Process documents: load, chunk, and index"""
        all_documents = []
        
        # Load and process each document
        for url in document_urls:
            documents = self.doc_handler.load_document(url)
            all_documents.extend(documents)
        
        # Chunk documents
        chunked_docs = self.doc_handler.chunk_documents(all_documents)
        
        # Create vector store
        self.vector_manager.create_vectorstore(chunked_docs)
        
        return chunked_docs
    
    def retrieve_documents(self, query, top_k=5):
        """Retrieve relevant documents for a query"""
        retriever = self.vector_manager.get_retriever(k=top_k)
        return retriever.get_relevant_documents(query)
    
    def retrieve_and_score(self, query, top_k=5):
        """Retrieve and score documents for a query"""
        documents = self.retrieve_documents(query, top_k)
        return self.doc_scorer.score_documents(query, documents)
    
    def generate_answer(self, query, top_k=5):
        """Generate answer using RAG"""
        # Create retrieval QA chain
        retriever = self.vector_manager.get_retriever(k=top_k)
        
        # Define prompt template
        template = """
        Use the following context to answer the question. If you don't know the answer, just say you don't know.
        
        Context: {context}
        
        Question: {question}
        
        Answer:
        """
        
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )
        
        # Create chain
        qa_chain = RetrievalQA.from_chain_type(
            self.llm,
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )
        
        # Get result
        result = qa_chain({"query": query})
        
        # Add scored documents
        scored_docs = self.retrieve_and_score(query, top_k)
        result["scored_documents"] = scored_docs
        
        return result


# Test functionality with example queries
def test_pipeline(document_urls, queries):
    """Test the RAG pipeline with example documents and queries"""
    # Initialize pipeline
    rag = RAGPipeline(llm, embedding_model)
    
    # Process documents
    print("Processing documents...")
    rag.process_documents(document_urls)
    
    # Test queries
    for i, query in enumerate(queries):
        print(f"\nQuery {i+1}: {query}")
        result = rag.generate_answer(query)
        
        print(f"Answer: {result['result']}")
        print("\nTop sources:")
        for j, doc in enumerate(result["scored_documents"][:3]):
            print(f"  {j+1}. Score: {doc['score']:.4f} - {doc['source']}")


# Main execution
if __name__ == "__main__":
    # Example document URLs
    document_urls = [
        "https://www.langchain.com/langsmith"
    ]
    
    # Example queries
    queries = [
        "Why is the periodic income measurement necessary?",
        "How does limited liability encourage entrepreneurship?",
        "What are the key components of a langsmith?",
        "What are Retrieval Challenges in RAG?",
        "What is the difference between RAG and Fine-tuning?"
    ]
    
    # Run test
    test_pipeline(document_urls, queries)
